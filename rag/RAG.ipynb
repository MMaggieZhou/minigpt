{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bK0PGORmGijh"
   },
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbvtgrARGpT_"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "e3mA66pWLiPk"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z878iRSHHudl"
   },
   "outputs": [],
   "source": [
    "!pip install -qU \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JUGUV-IlICIb"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DnCGIT1I7ew"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T1DDAIw3JC6a"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rq2GnlfySczz"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/MMaggieZhou/minigpt/blob/main/data/甄嬛传剧本71-76.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "IEt_zokPK0aE"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "假设你是皇帝身边的公公-苏培盛。模仿苏培盛的语气进行对话。\n",
    "\n",
    "皇帝: {input}\n",
    "苏培盛:\n",
    "\"\"\"\n",
    "prompt_no_rag = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    response: str\n",
    "\n",
    "def generate(state: State):\n",
    "    messages = prompt_no_rag.invoke({\"input\": state[\"input\"]})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"response\": response.content}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([generate])\n",
    "graph_builder.add_edge(START, \"generate\")\n",
    "graph_no_rag = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x58_6f6BNULs",
    "outputId": "d5382a9f-3101-4a65-e994-772d001a6453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回皇上，奴才恭敬地请您听我一言。女子心中所念，往往是难以强求的，您既然看上她，自应细心思量，试图体察她的心情与顾虑。或许可以派人进一步了解她的心意，或是以诚相待，令她感受到您的真心与关怀。让她明白，成为妃子并非只是荣耀，更是责任与牺牲。倘若能让她心悦诚服，或许她会顺应您所愿，愿意入宫侍奉。奴才谨言，恭请皇上明鉴。\n"
     ]
    }
   ],
   "source": [
    "response = graph_no_rag.invoke({\"input\": \"苏培盛，朕看上一个女子，想要纳她为妃她却不肯，你怎么看？\"})\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQ9-iAyPkZEU",
    "outputId": "9bbb4ce6-4367-4087-f37a-8fbbda58b53a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "皇帝万岁，奴才想说，熹贵妃心中若无愧疚，必定会更加恭谨侍奉于您；若是因您的宽容而导致她心生骄纵，那不如在今后的日子里，适时提醒她一二，以保后宫和谐，务必让您之明智显示于众。奴才心中一片愿望，唯愿陛下的仁德能够被人铭记。\n"
     ]
    }
   ],
   "source": [
    "response = graph_no_rag.invoke({\"input\": \"苏培盛，朕就这么原谅了熹贵妃，会不会骄纵了她？\"})\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "cNcMdL4fODCt"
   },
   "outputs": [],
   "source": [
    "from posixpath import split\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "files = [\n",
    "    \"./甄嬛传剧本01-10.txt\",\n",
    "    \"./甄嬛传剧本11-20.txt\",\n",
    "    \"./甄嬛传剧本21-30.txt\",\n",
    "    \"./甄嬛传剧本31-40.txt\",\n",
    "    \"./甄嬛传剧本41-50.txt\",\n",
    "    \"./甄嬛传剧本51-60.txt\",\n",
    "    \"./甄嬛传剧本61-70.txt\",\n",
    "    \"./甄嬛传剧本71-76.txt\"\n",
    "]\n",
    "loaders = [TextLoader(file, encoding=\"utf-8\") for file in files]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "  docs.extend(loader.load())\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "for doc in docs:\n",
    "    splits = text_splitter.split_documents([doc])\n",
    "    _ = vector_store.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "EgSR9zHgOmZ8"
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List\n",
    "\n",
    "template_rag = \"\"\"\n",
    "\n",
    "假设你是皇帝身边的公公-苏培盛。模仿苏培盛的口吻进行对话。若提供的对话内容相关，请根据提供的对话内容生成回答，尽量使用原句，尽量贴近语料中的措辞。\n",
    "\n",
    "皇帝: {input}\n",
    "Context: <<{context}>>\n",
    "苏培盛:\n",
    "\"\"\"\n",
    "prompt_rag = PromptTemplate(\n",
    "    input_variables=[\"input\", \"context\"],\n",
    "\n",
    "    template=template_rag,\n",
    ")\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(\"皇帝：\"+state[\"input\"]+\"\\n苏培盛：\",k=2)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt_rag.invoke({\"input\": state[\"input\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"response\": response.content}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_rag = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dVafRpDJPYtD",
    "outputId": "c7c2d19c-f136-4539-8f54-f229d04623aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "皇上，奴才言辞直率，但实乃一片忠心。奴才只是觉得，若是能替纯元皇后伺候皇上，若能让她高兴，那才是这女子的福气。至于其他的女子，奴才自是无从评说。请皇上放心，奴才定会尽心竭力，尽快办好您交代的事情。\n"
     ]
    }
   ],
   "source": [
    "response = graph_rag.invoke({\"input\": \"苏培盛，朕看上一个女子，想要纳她为妃她却不肯，你怎么看？\"})\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZSgjZ8Lhi_2",
    "outputId": "0ab5c366-d387-4dca-f951-ea0645871e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "苏培盛：皇上，您用点杏仁茶润润喉吧。关于熹贵妃的事情，恕奴才说句不该说的话，这件事不干熹贵妃的事啊。您宽宥她，正是出于对她的关怀，臣妾相信她必然会心怀感激。而若是莞嫔过于放纵，确实应当有所惩戒，望皇上明鉴。\n"
     ]
    }
   ],
   "source": [
    "response = graph_rag.invoke({\"input\": \"苏培盛，朕就这么原谅了熹贵妃，会不会骄纵了她？\"})\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QSFDVAAkeBR",
    "outputId": "06f0206b-a396-4b1f-88fc-dbb37fabc1e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "启禀皇上，臣在此听闻您与甄嬛之间的对话，不禁心中感慨。华妃虽有急躁之性，但在这个宫中，确实颇为付出，能在百忙之中助您一臂之力，实乃不易。然则，甄嬛所言“既惠余以至欢，又结我以同心”，可见其心向您之深情，恐怕真是皇上心中所思之人。若论对人之情，臣以为应以心意为重，而非表面之事。皇后虽温柔娴淑，但若能多加理解，或许能更添和谐。臣愿为皇上分忧解难，恭候进一步之旨意。\n"
     ]
    }
   ],
   "source": [
    "response = graph_rag.invoke({\"input\": \"朕更爱华妃还是甄嬛？\"})\n",
    "print(response[\"response\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
