{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPfKm/sg4/cXKit6064pseC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMaggieZhou/minigpt/blob/main/gpu_env.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDCYICpZTuPz",
        "outputId": "019302f3-d06e-4137-9b6b-edef72fd0e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'minigpt'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 93 (delta 47), reused 71 (delta 28), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (93/93), 740.87 KiB | 6.56 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/minigpt\n",
        "!git clone https://github.com/MMaggieZhou/minigpt.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/minigpt\")\n",
        "BaseDir  = \"/content/minigpt\""
      ],
      "metadata": {
        "id": "67T3TmZbTz-N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from execute import execute, generate\n",
        "from execute import set_device\n",
        "set_device(\"cuda\")"
      ],
      "metadata": {
        "id": "kB8eBVXfT4K8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\n",
        "    \"/content/minigpt/甄嬛传剧本01-10.txt\",\n",
        "    \"/content/minigpt/甄嬛传剧本11-20.txt\",\n",
        "    \"/content/minigpt/甄嬛传剧本21-30.txt\",\n",
        "    \"/content/minigpt/甄嬛传剧本31-40.txt\",\n",
        "    \"/content/minigpt/甄嬛传剧本41-50.txt\",\n",
        "    \"/content/minigpt/甄嬛传剧本51-60.txt\",\n",
        "    \"/content/minigpt/甄嬛传剧本61-70.txt\",\n",
        "    \"/content/minigpt/甄嬛传剧本71-76.txt\"\n",
        "\n",
        "]\n",
        "print(len(files))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59g5gfDBUBZX",
        "outputId": "6cc6a5d5-8b2f-4b6e-cb5b-544e67cf482c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data import load_data\n",
        "_, encoder = load_data(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0km7D4jiBFS",
        "outputId": "d3a67cc1-762d-4c41-b591-0ae6cec08164"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 248968 bytes\n",
            "Number of characters in the text: 85746\n",
            "File size: 237572 bytes\n",
            "Number of characters in the text: 81646\n",
            "File size: 243361 bytes\n",
            "Number of characters in the text: 83567\n",
            "File size: 235721 bytes\n",
            "Number of characters in the text: 80849\n",
            "File size: 220294 bytes\n",
            "Number of characters in the text: 75646\n",
            "File size: 227104 bytes\n",
            "Number of characters in the text: 77872\n",
            "File size: 224649 bytes\n",
            "Number of characters in the text: 77099\n",
            "File size: 109998 bytes\n",
            "Number of characters in the text: 37872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, idx_to_char = execute(files, BaseDir, dmodel=256, dff=1024, dk=128, seq_length=500, batch_size=50, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "GlSm54GrUCL0",
        "outputId": "14f4864a-91cb-478a-cfce-177055a3771b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 248968 bytes\n",
            "Number of characters in the text: 85746\n",
            "File size: 237572 bytes\n",
            "Number of characters in the text: 81646\n",
            "File size: 243361 bytes\n",
            "Number of characters in the text: 83567\n",
            "File size: 235721 bytes\n",
            "Number of characters in the text: 80849\n",
            "File size: 220294 bytes\n",
            "Number of characters in the text: 75646\n",
            "File size: 227104 bytes\n",
            "Number of characters in the text: 77872\n",
            "File size: 224649 bytes\n",
            "Number of characters in the text: 77099\n",
            "File size: 109998 bytes\n",
            "Number of characters in the text: 37872\n",
            "Vocabulary size: 3480\n",
            "Number of parameters in the model: 8759704\n",
            "Feature batch shape: torch.Size([50, 500])\n",
            "Labels batch shape: torch.Size([50, 500])\n",
            "Starting epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7756\n",
            "Starting epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-2759533781.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseDir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/minigpt/execute.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(train_data_files, output_dir, dmodel, h, dk, dff, num_layers, seq_length, batch_size, epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of parameters in the model: {num_params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgpt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/minigpt/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, inputs, targets, base_dir, batch_size, epochs, lr, weight_decay, device)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# persist the model to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# Save the model state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'/model_epoch_{epoch+1}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0;31m# .cpu() on the underlying Storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m                 \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;34m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load a model from local file\n",
        "\n",
        "import torch\n",
        "from model import GPTModel\n",
        "\n",
        "model_epoch_1 = GPTModel(vocab_size=3480, dmodel=256, dff=1024, dk=128, h=8, num_layers=6)\n",
        "model_epoch_1.to('cuda')\n",
        "\n",
        "# Define the path to your .pth file\n",
        "model_path = BaseDir + \"/model_epoch_1.pth\"\n",
        "\n",
        "# Load the state dictionary\n",
        "state_dict = torch.load(model_path)\n",
        "\n",
        "\n",
        "\n",
        "# Load the state dictionary into your model\n",
        "model_epoch_1.load_state_dict(state_dict)\n",
        "\n",
        "# Set the model to evaluation mode if you are loading for inference\n",
        "model_epoch_1.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg8hWLuugerN",
        "outputId": "d1be2d77-e402-48ce-dac5-19d70bb16489"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (embedding): Embedding(3480, 256)\n",
              "  (layers): ModuleList(\n",
              "    (0-5): 6 x AttentionLayer(\n",
              "      (self_attention): MultiHeadSelfAttention(\n",
              "        (Q): Linear(in_features=256, out_features=1024, bias=True)\n",
              "        (K): Linear(in_features=256, out_features=1024, bias=True)\n",
              "        (V): Linear(in_features=256, out_features=256, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "        (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (output_layer): Linear(in_features=256, out_features=3480, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model_epoch_1, encoder, \"第2000幕\", max_length=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g05IOLIIh0p_",
        "outputId": "b5cf9fde-ae49-4ffe-b56f-2313207b3578"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: 第2000幕\n",
            "（养心殿）\n",
            "皇帝：怎么了？\n",
            "甄嬛：臣妾恭送皇上。\n",
            "\n",
            "第302幕\n",
            "（养心殿）\n",
            "皇帝：这次？\n",
            "甄嬛：臣妾恭送皇上。\n",
            "皇帝：无妨，朕已经去陪你妃。\n",
            "甄嬛：臣妾恭送皇上。\n",
            "\n",
            "第303幕\n",
            "（皇帝已死，大牢）\n",
            "小厦子：皇上，小主在里头一个一个人。\n",
            "皇帝：你看着，这是什么缘故，最要此下去，朕知道你不是什么意思。\n",
            "甄嬛：臣妾不敢这样称赞？\n",
            "皇帝：你不敢妄言，朕若不信随你，岂不会怪罪？\n",
            "甄嬛：臣妾以为这样大清受之苦，只因为皇上所有一问吗？\n",
            "皇帝：从前你有没有说过。\n",
            "甄嬛：臣妾害怕。\n",
            "皇帝：那你的连年羹尧的心思都会说，朕信与你十分在一念之间，未必也是情分内人替朕？\n",
            "甄嬛：皇上，臣妾没有错，皇上要罚明君，便是因为臣妾和姐姐之缘故，才假孕争宠，难怪罪臣妾这样都不好。\n",
            "皇帝：好了，朕都是从前在想过于从前的事，朕都没有好好分寸，但愿不说就罢了。\n",
            "甄嬛：皇上是可怜爱公主，公主也就罢了，饶命人，臣妾这里不知道的事，除了皇上，就没有别人怕。\n",
            "皇帝：就没有朕。\n",
            "甄嬛：那臣妾也不敢。\n",
            "皇帝：你去看看吧。（举杯）你给甄嬛嬛，端妃也请安，朕与臣妾一样恭敬不敬。\n",
            "\n",
            "第136幕\n",
            "（碧桐书院）\n",
            "槿汐：皇上……\n",
            "皇帝：来了。\n",
            "浣碧姐\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/minigpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEGBzS6zhYf7",
        "outputId": "f3340bcd-1c66-49dd-e87f-a1d47b38fe2d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check.py\t model_epoch_1.pth  test.py\t\t 甄嬛传剧本31-40.txt\n",
            "data.py\t\t model_epoch_2.pth  train.py\t\t 甄嬛传剧本41-50.txt\n",
            "execute.py\t model.py\t    甄嬛传剧本01-10.txt  甄嬛传剧本51-60.txt\n",
            "gpu_env_2.ipynb  __pycache__\t    甄嬛传剧本11-20.txt  甄嬛传剧本61-70.txt\n",
            "gpu_env.ipynb\t sample.txt\t    甄嬛传剧本21-30.txt  甄嬛传剧本71-76.txt\n"
          ]
        }
      ]
    }
  ]
}